# hybridrag-v1

A hybrid chatbot backend built with **FastAPI**, **Redis (via Upstash)**, and a **PDF-based RAG system**. This project does **not rely on an LLM for response generation**, but instead retrieves accurate answers directly from the indexed PDF content using a **combination of BM25 and semantic embeddings**. Designed for contextual, cache-aware interactions, the system uses Redis to minimize latency by caching frequent queries and avoiding redundant LLM calls — resulting in a much faster and more scalable experience.

Optional **guardrails** enforce strict retrieval behavior to maintain answer reliability and safety.


## ✨ Features

- 📄 **PDF Knowledge Base**: The system extracts and indexes content from PDF documents
- 🔍 **Hybrid RAG Retrieval**: Combines BM25 (sparse) and `all-MiniLM-L6-v2` (dense) for robust, context-aware search
- 🧠 **No LLM Generation**: Answers are retrieved — not generated — ensuring reliability and avoiding hallucinations
- 🧱 **Guardrails**: Optional constraints ensure answers are based strictly on known sources
- ⚡ **FAISS for Fast Vector Search**: Efficient in-memory vector lookup
- 🗂️ **Redis via Upstash**: Caches query-response pairs to reduce latency
- 🌐 **FastAPI**: REST API backend for integration with frontends like WhatsApp or web apps
- ☁️ **Deployed on Railway**: Cloud deployment with free-tier hosting


## 🚀 Use Cases

- 📄 **PDF-Based Question Answering**: Answers are directly extracted or retrieved from PDF content — not generated by the LLM
- 🔍 **Hybrid Retrieval System**: Uses both keyword-based (BM25) and embedding-based (dense) methods for highly accurate search
- 🧠 **Semantic Search over Documents**: Allows users to query natural language over document data
- 🧱 **Guardrail-Controlled Responses**: Ensures safe, predictable answers by avoiding open-ended LLM completions
- ⚡ **Low-Latency Responses**: Redis caching drastically reduces response time for repeated queries



## 🛠️ Tech Stack

- Python
- FastAPI
- FAISS
- Redis (Upstash)
- SentenceTransformers
- Gemini API
- Railway (deployment)

## 📦 How It Works

1. User sends a query to the FastAPI endpoint
2. Redis checks if a cached result exists
3. If not, embeddings are generated via SentenceTransformer
4. FAISS retrieves relevant context
5. Google API completes the final response
6. The result is cached in Redis for future hits

---


